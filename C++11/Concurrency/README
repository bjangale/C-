// Example Detaching thread to handle other doucment document
void edit_document(std::string const &filename)
{
    open_document_and_display_gui(filename);
    while (!done_editing())
    {
        user_command cmd = get_user_input();
        if (cmd.type == open_new_document)
        {
            std::string const new_name = get_filename_from_user();
            // if the user chooses to open new document you prompt them
            // for document to open start a new thread to open that
            // document and then detach it.
            std::thread t(edit_document, new_name);
            t.detach();
        }
        else
        {
            process_user_input(cmd);
        }
    }
}

######################################################################################################
Mutexes : Mutexes are used to prevent data races by ensuring that only one thread can access critical
          section of code at a time.
1. std::mutex mtx;
    mtx.lock()      - locks the mutex if available.if locked calling thread owns the mutex until it
                      calls unlock()
    mtx.unlock()    - unlocks the mutex. 
    mtx.try_lock()  - try to locks the mutex, return true if locked successfully false otherwise
    mtx.native_handle() - return platform-specific native handle. 

2. std::timed_mutex tmtx;
    tmtx.lock()     - locks the mutex, blocks if the mutex is not available
    tmtx.try_lock() - tries to lock the mutex, returns if the mutex is not available
    tmtx.try_lock_for() - tries to lock the mutex, returns if the mutex has been
                         unavailable for the specified timeout duration
    tmtx.try_lock_until() - tries to lock the mutex, returns if the mutex has been
                            unavailable until specified time point has been reached
    tmtx.unlock()       - unlocks the mutex

3. std::recursive_mutex - allows same thread to lock the mutex multiple times. 

4. std::lock_guard<std::mutex> guard(mtx)
                    - RAII style mechanism for owning a mutex for duration of scoped block.
                      provides convinient way to lock and unlock mutex automatically. 
5. std::lock(MutexTypes&... mtxs) 
                    - function in C++ is used to lock multiple mutexes simultaneously without 
                      causing deadlock. It ensures that all mutexes are locked or none are locked. 
                      If it cannot lock all mutexes, it will unlock any mutexes it has already locked and try again.          
5. std::unique_lock<std::mutex>  
      - unique_lock provides more flexlibility such as differed locking,
        time locking, manual unlocking.
      - if the instance does own the mutex, the destructor must call unlock()
        and if instance does not own the mutext then it must not call unlock()
      - std::unique_lock takes more space and is fraction slow to use than 
        std::lock_gaurd. 

    lock
	locks (i.e., takes ownership of) the associated mutex

    try_lock
	    tries to lock (i.e., takes ownership of) the associated mutex without blocking

    try_lock_for
	    attempts to lock (i.e., takes ownership of) the associated TimedLockable mutex, returns 
	    if the mutex has been unavailable for the specified time duration

    try_lock_until
	    tries to lock (i.e., takes ownership of) the associated TimedLockable mutex, returns if 
	    the mutex has been unavailable until specified time point has been reached

    unlock
	    unlocks (i.e., releases ownership of) the associated mutex

Modifiers:
    swap
	    swaps state with another std::unique_lock
    release
	    disassociates the associated mutex without unlocking (i.e., releasing ownership of) it
    Observers:
    mutex
	    returns a pointer to the associated mutex
    owns_lock
	tests whether the lock owns (i.e., has locked) its associated mutex 
                      
 Locking at appropriate granularity:
    - You may not need the mutex locke across the call to process, so you manually unlock it 
      before the call and then lock it again.  
        void get_and_process_data()
        {
            std::unique_lock<std::mutex> my_lock(the_mutex);
            some_class data_to_process=get_next_data_chunk();
            my_lock.unlock();
            result_type result=process(data_to_process);
            my_lock.lock();
            write_result(data_to_process,result);
        }

    - In general lock should be held for only the minimum possible time needed to perform the 
      required operations. 
    - This also means that time consuming operations such as acquiring another lock or waiting 
      for I/O to complete shouldn't be done while holding the lock unless absolutely neccessary. 
    - Sometimes if you don't hold required locks for the entire duration of an operation, you are
      exposing yourself to race condition. 

6. Protecting share data during initialization
    - Lazy initialization is common in single threaded code each operation that require resource
      first check to see if resource is initialized and then initialize it before use if not. 
        std::shared_ptr<some_resource> resource_ptr;
        void foo()
        {
            if(!resource_ptr)
            {
                resource_ptr.reset(new some_resource);
            }
            resource_ptr->do_something();
        }
    - If the shared resouce itself is safe for concurrent access, the only part that needs to be
      protecting when converting this this to multithreaded code is initialization. 
      Naive translation such as following listing can cause unneccesary serialization of thread 
      using the resource. 

        std::shared_ptr<some_resource> resource_ptr;
        std::mutex resource_mutex;
        void foo()
        {
            std::unique_lock<std::mutex> lk(resource_mutex);
            if(!resource_ptr)
            {
                resource_ptr.reset(new some_resource);
            }
            lk.unlock();
            resource_ptr->do_something();
        }
    - The better way to do this is, inclusing Double-Checked Locking pattern. 
        void undefined_behaviour_with_double_checked_locking()
        {
            if(!resource_ptr)
            {
                std::lock_guard<std::mutex> lk(resource_mutex);
                if(!resource_ptr)
                {
                    resource_ptr.reset(new some_resource);
                }
            }
            resource_ptr->do_something();
        }
      It has potential for nasty race condtion because the read outside the lock is not 
      synchronised with the write done by another thread inside the lock. C++ standard 
      library provides std::once_flag and std::call_once to hand this situation. 
    -std::once_flag
        Rather than locking a mutex and explicitly checking the pointer, every
        thread can just use std::call_once, safe in the knowledge that the pointer will have
        been initialized by some thread (in a properly synchronized fashion) by the time
        std::call_once returns. Use of std::call_once will typically have a lower overhead
        than using a mutex explicitly, especially when the initialization has already been
        done, so should be used in preference where it matches the required functionality.

        std::shared_ptr<some_resource> resource_ptr;
        std::once_flag resource_flag;
        void init_resource()
        {
            resource_ptr.reset(new some_resource);
        }
        void foo()
        {
            std::call_once(resource_flag,init_resource);
            resource_ptr->do_something();
        }

######################################################################################################
1. std::condition_variable :
    std::condition_variable and std::condition_variable_any. Both of
    these are declared in the <condition_variable> library header. In both cases, they
    need to work with a mutex in order to provide appropriate synchronization; the former 
    is limited to working with std::mutex, whereas the latter can work with anything
    that meets some minimal criteria for being mutex-like, hence the _any suffix. Because
    std::condition_variable_any is more general, there’s the potential for additional
    costs in terms of size, performance, or operating system resources, so std::condition_
    variable should be preferred unless the additional flexibility is required.

    Notification:
    notify_one
        notifies one waiting thread
    notify_all
        notifies all waiting threads
    Waiting:
        wait
            blocks the current thread until the condition variable is awakened
        wait_for
            blocks the current thread until the condition variable is awakened or 
            after the specified timeout duration
        wait_until
            blocks the current thread until the condition variable is awakened or 
            until specified time point has been reached
    Native handle:
        native_handle
            returns the native handle

    void wait( std::unique_lock<std::mutex>& lock );
    template< class Predicate >
    void wait( std::unique_lock<std::mutex>& lock, Predicate pred );
    predicate is called while holding the lock; if condition is satisfied (predicate return true) 
    it return with holding the lock, if condition isn't satisfied wait() unlocks the mutex and puts
    the thread in a blocked or waiting state. 

    If the condition hasn’t been satisfied, the thread unlocks the
    mutex and resumes waiting. This is why you need the std::unique_lock rather than
    the std::lock_guard—the waiting thread must unlock the mutex while it’s waiting
    and lock it again afterward, and std::lock_guard doesn’t provide that flexibility. 
    If the mutex remained locked while the thread was sleeping, the data-preparation
    thread wouldn’t be able to lock the mutex to add an item to the queue, and the waiting 
    thread would never be able to see its condition satisfied

    During a call to wait(), a condition variable may check the
    supplied condition any number of times; however, it always does so with the mutex
    locked and will return immediately if (and only if) the function provided to test the
    condition returns true.

2. Waiting for one-off event with future:
	The C++ Standard Library models this sort of one-off event with something called
	a future. If a thread needs to wait for a specific one-off event, it somehow obtains a
	future representing this event. The thread can then periodically wait on the future for
	short periods of time to see if the event has occurred (check the departures board)
	while performing some other task (eating in the overpriced café) in between checks.
	Alternatively, it can do another task until it needs the event to have happened before it
	can proceed and then just wait for the future to become ready. A future may have data
	associated with it (such as which gate your flight is boarding at), or it may not. Once an
	event has happened (and the future has become ready), the future can’t be reset.
	There are two sorts of futures in the C++ Standard Library, implemented as two
	class templates declared in the <future> library header: unique futures (std::future<>)
	and shared futures (std::shared_future<>). These are modeled after std::unique_ptr
	and std::shared_ptr. An instance of std::future is the one and only instance that
	refers to its associated event, whereas multiple instances of std::shared_future may
	refer to the same event. In the latter case, all the instances will become ready at the
	same time, and they may all access any data associated with the event. 

3. Returning values from background task:
	You could start a new thread to perform the calculation,but that means you have to take 
    care of transferring the result back, because std::thread doesn’t provide a direct mechanism 
    for doing so. This is where the std::async function template (also declared in the 
    <future> header) comes in. You use std::async to start an asynchronous task for which you 
    don’t need the result right away. Rather than giving you back a std::thread object to wait on,
	std::async returns a std::future object, which will eventually hold the return value
	of the function. When you need the value, you just call get() on the future, and the
	thread blocks until the future is ready and then returns the value. The following listing
	shows a simple example.
    
    #include <future>
    #include <iostream>
    int find_the_answer_to_ltuae();
    void do_other_stuff();
    int main()
    {
        std::future<int> the_answer=std::async(find_the_answer_to_ltuae);
        do_other_stuff();
        std::cout<<"The answer is "<<the_answer.get()<<std::endl;
    }

    std::async allows you to pass additional arguments to the function by adding extra
    arguments to the call, in the same way that std::thread does. If the first argument is
    a pointer to a member function, the second argument provides the object on which to
    apply the member function (either directly, or via a pointer, or wrapped in std::ref), 
    and the remaining arguments are passed as arguments to the member functi`on. 

	By default, it’s up to the implementation whether std::async starts a new thread, or
	whether the task runs synchronously when the future is waited for. In most cases this is
	what you want, but you can specify which to use with an additional parameter to
	std::async before the function to call. This parameter is of the type std::launch, and
	can either be std::launch::deferred to indicate that the function call is to be
	deferred until either wait() or get() is called on the future, std::launch::async to
	indicate that the function must be run on its own thread, or std::launch::deferred |
	std::launch::async to indicate that the implementation may choose. This last option
	is the default. If the function call is deferred, it may never actually run. For example:
	
    auto f6=std::async(std::launch::async,Y(),1.2); <- run in new thread
	auto f7=std::async(std::launch::deferred,baz,std::ref(x)); <- run in wait() or get()
	auto f8=std::async(
	std::launch::deferred | std::launch::async,
	baz,std::ref(x)); <- implementation chooses
	auto f9=std::async(baz,std::ref(x));
	f7.wait();   <- invokes differed function
	
    The use of std::async makes it easy to divide algorithms into tasks that can be run concurrently

4. Associating a task with a future
    std::packaged_task<> ties a future to a function or callable object. When the std::
	packaged_task<> object is invoked, it calls the associated function or callable object
	and makes the future ready, with the return value stored as the associated data. This
	can be used as a building block for thread pools (see chapter 9) or other task management 
    schemes, such as running each task on its own thread, or running them all
	sequentially on a particular background thread. If a large operation can be divided into
	self-contained sub-tasks, each of these can be wrapped in a std::packaged_task<>
	instance, and then that instance passed to the task scheduler or thread pool. This
	abstracts out the details of the tasks; the scheduler just deals with std::packaged_
	task<> instances rather than individual functions.
	
    The template parameter for the std::packaged_task<> class template is a function signature, 
    like void() for a function taking no parameters with no return value,
	or int(std::string&,double*) for a function that takes a non-const reference to a
	std::string and a pointer to a double and returns an int. When you construct an
	instance of std::packaged_task, you must pass in a function or callable object that
	can accept the specified parameters and that returns a type convertible to the specified 
    return type. The types don’t have to match exactly; you can construct a std::packaged_task<double(double)> 
    from a function that takes an int and returns a float because the types are implicitly convertible.
	The return type of the specified function signature identifies the type of the std::future<> returned 
    from the get_future() member function, whereas the argument list of the function signature is used 
    to specify the signature of the packaged task’s function call operator.
    
    template<>
    class packaged_task<std::string(std::vector<char>*,int)>
    {
       public:
       template<typename Callable>
       explicit packaged_task(Callable&& f);
       std::future<std::string> get_future();
       void operator()(std::vector<char>*,int);
    }

######################################################################################################
1. A simple single-threaded queue implementation
    void push(T new_value)
    {
        std::unique_ptr<node> p(new node(std::move(new_value)));
        node* const new_tail=p.get();
        if(tail)
        {
            tail->next=std::move(p);
        }
        else
        {
            head=std::move(p);
        }
        tail=new_tail;
    }

    The most obvious problem is that push() can modify both head f and tail g,
    so it would have to lock both mutexes. This isn’t too much of a problem, although
    it’s unfortunate, because locking both mutexes would be possible. The critical problem 
    is that both push() and pop() access the next pointer of a node: push() updates
    tail->next e, and try_pop() reads head->next d. If there’s a single item in the
    queue, then head==tail, so both head->next and tail->next are the same object,
    which therefore requires protection. Because you can’t tell if it’s the same object 
    without reading both head and tail, you now have to lock the same mutex in both push()
    and try_pop(), so you’re no better off than before. Is there a way out of this dilemma?

    ENABLING CONCURRENCY BY SEPARATING DATA
    You can solve this problem by preallocating a dummy node with no data to ensure that
    there’s always at least one node in the queue to separate the node being accessed at
    the head from that being accessed at the tail. For an empty queue, head and tail now
    both point to the dummy node rather than being NULL. This is fine, because
    try_pop() doesn’t access head->next if the queue is empty. If you add a node to the
    queue (so there’s one real node), then head and tail now point to separate nodes, so
    there’s no race on head->next and tail->next.

    template<typename T>
    class queue
    {
        private:
        struct node
        {
            std::shared_ptr<T> data;
            std::unique_ptr<node> next;
        };
        std::unique_ptr<node> head;
        node* tail;
        public:
        queue():head(new node),tail(head.get()){}
        queue(const queue& other)=delete;
        queue& operator=(const queue& other)=delete;
        
        std::shared_ptr<T> try_pop()
        {
            if(head.get()==tail)
            {
                return std::shared_ptr<T>();
            }
            std::shared_ptr<T> const res(head->data);
            std::unique_ptr<node> old_head=std::move(head);
            head=std::move(old_head->next);
            return res;
        }
    
        void push(T new_value)
        {
            std::shared_ptr<T> new_data(
            std::make_shared<T>(std::move(new_value)));
            std::unique_ptr<node> p(new node);
            tail->data=new_data;
            node* const new_tail=p.get();
            tail->next=std::move(p);
            tail=new_tail;
        }
    }

    The new node you create is going to be the new dummy node, so you don’t need 
    to supply the new_value to the constructor i. Instead, you set the data on
    the old dummy node to your newly allocated copy of the new_value
    I’m sure you’re wondering what these changes buy you and how they help
    with making the queue thread-safe. Well, push() now accesses only tail, not head,
    which is an improvement. try_pop() accesses both head and tail, but tail is
    needed only for the initial comparison, so the lock is short-lived. The big gain 
    is that the dummy node means try_pop() and push() are never operating on the same
    node, so you no longer need an overarching mutex. So, you can have one mutex for
    head and one for tail.

    